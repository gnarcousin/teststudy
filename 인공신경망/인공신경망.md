<h1>20차시 - 자기 조직화 지도</h1>

<h2> 20.1 (self - organazing - map) SOM </h2>
자율 신경망 = 인간의 자율적인 학습과 유사한 형태로 학습이 이루어지는 신경망

*SOM과 ART로 나뉘는데 SOM만 시험에 나옴*

SOM
- 자율 신경망의 대표적 모델
- 비지도 학습에 의한 클러스터링 방법 중의 하나
- 고차원의 데이터들을 줄여서 가시화하는 방법
  (4D, 3D >> 2D)
- 1980년대 튜브 코호넨에 의해 고안됨

- 입력층과 출력층으로 구성된 순방향 단층 신경망 구조
- 출력층 뉴런을 2차원으로 배열함 (사각형 배열, 육각형 배열 형태가 가능)

<h2>20-2. 자기조직화 지도의 학습 알고리즘</h2>

SOM에서는 입력 패턴과 가장 유사한 가중치를 갖는 출력층 뉴런을 Winner 뉴런으로 설정함

- SOM의 학습 방법 = Winner 뉴런이 J라고 했을 때 J를 기준으로 반경 r을 설정 > r 범위 내의 뉴런들의 가중치를 업데이트함

<h1> 23차시 - 패턴의 유사도 </h1>

<h2> 23.1 경쟁식 신경망 </h2>

경쟁식 신경망 = 패턴의 유사도 측정 도구

*경쟁식 신경망은 Hamming Net, CP Net으로 나뉨*
*유사도 측정 도구는 해밍 거리, 유클리드 거리로 나뉨*

패턴의 유사도 = 패턴들 사이의 일치하는 정도를 정량적(수치적)으로 표현한 것

해밍 거리 = 패턴을 비교하였을 때 패턴 사이의 값이 다른 비트들의 갯수 (해밍 거리가 작을수록 유사함)
- 단순 비트를 비교하는 방식임
  
유클리드 거리 = A와 B의 직선 상의 거리 (우리가 통상적으로 생각하는 거리)

*해밍거리, 유클리드 거리는 연습 문제 꼭 알아둘 것(풀어볼 것)*

<h1> 24차시 Hamming Net </h1>

<h2> 24.1 Hamming Net </h2>

Hamming Net = 해밍 거리를 이용해 표본 패턴을 식별하는 신경망

- 입력 패턴이 n개의 픽셀이고 표본 패턴 수가 m이면, 입력층 / 출력층은 각각 n / m 개의 뉴런들로 구성됨
- 표본 패턴의 개수가 1개일 경우 가중치 W를 별도로 학습하지 않고 표본 패턴 s로부터 직접 구할 수 있음
  
<h2> 24.2 Hamming Net의 표본 패턴 저장 </h2>

*Hamming Net 연습 문제 무조건 풀어보기*

입력 패턴과 가장 유사한 표본 패턴을 찾는 법
= Hamming net은 경사 함수를 (Zi/n) 사용함
f(z) > y

출력층 뉴런들 중 출력값이 가장 큰 뉴런의 색인(index) J를 찾아내면 그것이 가장 유사한 패턴이 됨

*Hamming Net 연습문제도 풀어보기*

<h1> 29차시 오류 역전파 알고리즘</h1>

<h2> BP 알고리즘</h2>

backpropagation (역전파) = error backpropagation(오류 역전파) = BP 알고리즘

BP 알고리즘 = 다층 퍼셉트론의 학습을 하는 방법 (효과적으로 가능해 가장 널리 사용되는 중임)

*모멘텀 BP 알고리즘은 시험에 안나옴 이 챕터는 개념이 중요함!*

- 출력층의 오차 신호를 이용해 (은닉층에 역전파하여) 입력층과 은닉층 사이의 가중치를 변경하는 학습 방법

*학습 절차, 연습 문제는 시험에 안나옴 개념만 알아두면 됨*

<h1> 30차시 학습 인자 </h1>

*시험 문제 많이 냄*

<h2> 30.1 초기 가중치 </h2>

BP 알고리즘
- 은닉층의 수가 많아지면 학습이 매우 느려짐
- 경우에 따라 학습이 이루어지지 않을 수도 있는 문제점 존재
- 오늘 날에는 다양한 방법으로 해결함으로서 심층 신경망을 널리 사용할 수 있게 됨

학습 인자
매개 변수(Parameter) / 하이퍼 파라메터 (Hyperparameter)

매개 변수(Parameter) = 학습에 의해 최적의 값이 결정되는 가중치와 바이어스(Bias)

하이퍼 파라메터(Hyperparameter) = 매개 변수 외의 학습을 효과적으로 수행하기 위해 설정해야 하는 많은 요소들

- 학습에 의해 찾을 수 없어 인간이 수작업으로 찾아줘야 함
- 만들 수 있는 조합의 경우의 수가 너무 많아 최적의 하이퍼 파라메터를 찾는 것은 어려움 > 많은 경험과 시행 착오를 통해 얻은 지식들을 바탕으로 그 값이 결정됨

초기 가중치 
- 신경망 학습에 있어서 가중치의 초기화는 매우 중요
- 가중치의 초기화를 잘못하면 오차가 커질 수 있음
- Xavier 초기화나 He 초기화 방법을 주로 사용함

<h2> 30.2 은닉층의 수</h2>

은닉층의 수는 사람이 결정하는 하이퍼 파라메터
- 은닉층의 수는 학습 시간에 상당한 영향을 주기 때문에 하이퍼 파라메터 설정이 중요함
- 아직 해답이 없기 떄문에 시행착오를 바탕으로 최적의 값을 찾아야 함

*연습 문제 꼭 풀어보기*

<h2> 30.3 비용 함수</h2>

lost function = cost function = 비용함수

기울기가 감소하는 방향(손실을 최소화 하는 방향)으로 학습(경사 하강법)이 진행됨
*편미분이 들어가는 내용은 시험에 안나옴*

<h2> 30.4 overfitting </h2>

오버피팅(overfitting)은 과적합이라고도 함

- 신경망이 학습 데이터에 과도하게 특화되어 학습함
- 학습에 사용된 데이터에 대해서는 성능이 좋으나 새로운 데이터에 대해서는 오히려 성능이 떨어지는 현상

언더피팅 = 학습이 너무 미진한 상황 (데이터에 덜 들어맞음)
적절하게 학습됨 = 오류는 발생할 수 있으나 적절하게 학습되어있음
오버피팅 = 과도하게 학습이 이루어짐 (데이터를 엄격하게 구분함)

- 일반적으로 학습 데이터가 너무 적거나 심층 신경망의 은닉층의 수가 너무 많아서 가중치 수가 많아져 오버피팅이 발생함
- 해결을 위해 정규화 / 드롭 아웃 등의 기법이 개발됨

정규화 = L1 정규화 / L2 정규화로 구분됨

- 페널티라는 항목을 비용 함수에 추가해 특정 가중치가 지나치게 커지는 것을 억제해 오버 피팅을 방지하는 방식임

드롭 아웃

- 입력층과 은닉층의 일부 뉴런들을 삭제하고 해당 뉴런들에 연결된 가중치들을 학습에서 제외시키는 방식
- 제외되는 뉴런들은 랜덤하게 결정됨
- 학습 단계에서만 적용하며 실제 응용에서는 모든 뉴런들을 다시 포함시켜서 사용함

*개념 정도만 이해하고 넘어가기*

<h1> 31차시 컨볼루션 신경망 </h1>

<h2> 31.1 특징 </h2>

인공신경망(Artificial Neural Network, ANN)

- 은닉층이 2개 이상인 다층 신경망 = Deep Neural Network, DNN = 심층 신경망
- 그 중 대표적인 모델이 컨볼루션 신경망(CNN)임

컨볼루션 신경망(Convolution Neural Network, CNN)

- 인간 뇌의 시각피질에서 영상을 처리하는 것과 유사한 기능을 하는 신경망
- 영상 인식 분야에 매우 유용하게 사용되고 있음
- 기존 신경망은 분류기만 신경망을 사용했으나 CNN은 특징 추출과 분류기 모두 신경망을 사용함

<h2> 31.2 구조 </h2>

특징 추출 신경망 / 분류 신경망으로 구현되어 있으며 분류 신경망은 일반적인 DNN, ANN과 같음

- 특징 추출 신경망은 컨볼루션 계층 / 풀링 계층이 존재함
- 컨볼루션 계층 = 필터를 통해 입력 영상의 지역적인 특징을 뽑아냄
- 풀링 계층 = 뽑아낸 특징 중 대표값을 뽑아내 평균값을 만들어내는 계층
- 분류 신경망 = 모든 뉴런들이 연결되어 있어 완전 연결 계층 (Fully Connected, FC 계층)이라고도 부름 
- 평균 풀링 (평균값으로 대표값 뽑아냄) / 최대 풀링(최댓값으로 대표값을 뽑아냄) 방식이 존재함
- 일반적으로 컨볼루션 계층의 활성화 함수는 ReLU 함수임

*컨볼루션 계층과 연산, 특징 맵의 크기를 구하는 방법, 패딩 개념, 특징맵의 크기 계산, 연습문제 꼭 알아야 함*
*특히 연습문제 전부 풀어보기*

- 분류 신경망의 활성화 함수는 대부분 최종 출력층이므로 소프트맥스 함수를 사용하는 경향이 있음
- 심층 컨볼루션 신경망 = 컨볼루션 신경망의 구조가 반복되어있음