<h1>인공신경망</h1>

<h2> 20차시 - 자기 조직화 지도<h2>

<h2> 20-1. SOM (Self-Organazing-Map)</h2>

**자율신경망** = 자율적인 학습이 이루어지는 신경망

**SOM**
> 자율 신경망의 대표적인 모델
> **비지도 학습에 의한 클러스터링(군집화) 방법** 중 하나
> 고차원의 데이터들을 줄여서 가시화하는 방법
>> 3D, 4D >> 2D
> 1980년 튜브 코호넨에 의해서 고안됨 
>
> 입력층 / 출력층으로 구성된 **단층 신경망 구조**
> 출력층의 뉴런 = **2차원 배열**
>> 사각형 배열 / 육각형 배열 형태
 
![](../../1.png)

<h2> 20-2. 자기 조직화 지도의 학습 알고리즘</h2>

**SOM의 Winner 뉴런** = 입력 패턴과 가장 유사한 가중치를 갖는 출력층 뉴런  <br>

**SOM**의 학습 방법 = J(winner 뉴런)를 기준으로 반경 r을 설정해 **r 내의 뉴런들의 가중치**를 업데이트

<h2> 23차시 - 패턴의 유사도 </h2>

<h2> 23.1 경쟁식 신경망 </h2>

**경쟁식 신경망** = 패턴의 유사도 측정 도구 (해밍 거리 / 유클리드 거리)

> *Hamming Net / CP net으로 나뉨*  
> **패턴의 유사도** = 패턴들 사이의 일치하는 정도를 정량(수치)적으로 표현한 것  
>   
> 해밍 거리 = 비교된 패턴들의 사잇값이 다른 비트들의 갯수
>> 해밍 거리가 다를 수록 유사함  
>> 단순히 두 패턴의 비트를 비교하는 방식

<h2> 24차시 - Hamming Net </h2>

<h2> 24.1 Hamming Net </h2>

Hamming Net = 해밍 거리를 이용해 표본 패턴을 식별하는 신경망

> 입력 패턴이 n개의 픽셀이고 표본 패턴 수가 m개이면, **입 / 출력층은 각각 n / m개의 뉴런으로 구성**됨 
> ![](../../5.png)
> 표본 패턴의 개수가 1개인 경우 **가중치 W를 별도 학습하지 않고** 표본 패턴 s로부터 직접 값을 구할 수 있음

<h2> 24.2 Hamming Net의 표본 패턴 저장 </h2>

입력 패턴과 가장 유사한 표본 패턴을 찾는 법
> Hamming Net은 **경사 함수**를 사용함
> ![](../../21.png)
> 출력층 뉴런들 중 **출력값이 가장 큰 뉴런의 색인(index) J**를 찾음
>> 색인을 바탕으로 s(J)를 가장 유사한 표본 패턴으로 판단

<h2> 29차시 오류 역전파 알고리즘 </h2>

*BackPropagation (역전파) = error Backpropagation (오류 역전파)
= BP 알고리즘*    

**BP 알고리즘** = 다층 퍼셉트론의 학습 방법
> 효과적인 학습이 가능해 가장 널리 사용되는 방법임
> <br>
> 출력층의 오차 신호를 은닉층에 **역전파하여** 은닉층의 오차신호를 구함
>> 구한 은닉층의 **오차신호를 이용해** 입력층과 은닉층 사이의 가중치를 변경하는 방식
> ![](../../5-2.png)

<h2> 30차시 학습 인자 </h2>

<h2> 30.1 초기 가중치 </h2>

**BP 알고리즘의 문제점**
> 은닉층의 수가 많아지면 **학습이 매우 느려짐**
>> 경우에 따라 학습이 이루어지지 않을 수도 있음
> 오늘 날에는 다양한 방법으로 이를 해결해 심층 신경망을 널리 사용 가능

**학습 인자(매개 변수(Parameter) / 하이퍼 파라메터(Hyper Parameter))**

**매개 변수** = 학습에 의해 최적의 값이 결정되는 가중치와 바이어스(Bias)  

**하이퍼 파라메터(Hyperparameter)**
> 매개 변수 외의, 학습을 효과적으로 수행하기 위해 **설정해야하는 많은 요소들**
> **학습에 의해 찾을 수 없음** (인간이 수작업으로 찾아야 함)  
> 만들 수 있는 조합의 경우의 수가 매우 많아 최적의 파라미터를 찾기 어려움
>> 많은 경험과 시행 착오를 통해 얻은 **지식들을 바탕으로 값이 결정됨**
 
초기 가중치
> 신경망 학습에 있어서 **가중치의 초기화**는 매우 중요함
> 가중치의 초기화를 잘못하면 오차가 커질 수 있음
> Xavier 초기화나 He 초기화 방법을 주로 사용
 
<h2> 30.2 은닉층의 수</h2>
 
은닉층의 수는 사람이 결정하는 하이퍼 파라메터
> 학습 시간에 상당한 영향을 주기 때문에 하이퍼 파라메터 설정이 중요  
> 시행착오를 바탕으로 최적의 값 설정 필요
 
<h2> 30.3 비용 함수</h2>
 
*lost function = cost function = 비용 함수*
